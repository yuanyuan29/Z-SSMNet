[← Return to overview](https://github.com/yuanyuan29/Z-SSMNet/tree/master#z-nnmnet)

## Z-nnMNet

Considering the heterogeneous between data from multi-centres and multi-vendors, we integrated the zonal-aware mesh network into the famous nnU-Net framework [[1]](https://www.nature.com/articles/s41592-020-01008-z) which provides a performant framework for medical image segmentation to form the `Z-nnMNet` that can pre-process the data adaptively. For large datasets with labels, the model can be trained from scratch. If the dataset is small or some labels of the data are noisy, fine-tuning based on the SSL pre-trained model can help achieve better performance.

The changes we have made include:

1. **Network architecture:** upgrade the generic `U-Net` to `MNet`
2. **Input data:** integrate zonal masks of prostate as additional inputs
3. **Loss function:** replace the default Cross-Entropy + soft Dice loss in `nnU-Net` with Cross-Entropy + Focal loss for prostate lesion detection

### Z-nnMNet - Docker Setup

To run the commands, you can use the Docker specified in [`z_nnmnet/training_docker/`](https://github.com/yuanyuan29/Z-SSMNet/tree/master/src/z_ssmnet/z_nnmnet/training_docker). This Docker container shows how custom nnU-Net trainers can be implemented (shown for `MNet_myTrainer_zonal.py`).

To build the Docker container, navigate to [`z_nnmnet/training_docker/`](https://github.com/yuanyuan29/Z-SSMNet/tree/master/src/z_ssmnet/z_nnmnet/training_docker) and build the container:

```shell
cd src/z_ssmnet/z_nnmnet/training_docker/
docker build . --tag yuanyuan29/z-ssmnet:latest
```

This will result (if ran successfully) in the Docker container named `yuanyuan29/z-ssmnet:latest`. Alternatively, the pre-built Docker container can be loaded:

```shell
docker pull yuanyuan29/z-ssmnet:latest
```

### Z-nnMNet - Cross-Validation Splits

Z-nnMNet can train with user-defined cross-validation splits, which is required to ensure there is no patient overlap between training and validation splits. The [cross-validation splits](#cross-validation splits) can be copied to Z-nnMNet's working directory as follows:

```python
from z_ssmnet.splits.picai import nnunet_splits

nnUNet_splits_path = "/workdir/nnUNet_raw_data/Task2302_z-nnmnet/splits.json"

# save cross-validation splits to disk
with open(nnUNet_splits_path, "w") as fp:
    json.dump(nnunet_splits, fp)
```

Alternatively, the generated [splits.json](https://github.com/yuanyuan29/Z-SSMNet/blob/master/src/z_ssmnet/splits/picai/splits.json) can be copied directly.

### Z-nnMNet - Data pre-processing

* Process the raw images by the data pre-processing settings (`nnUNet_plan_and_preprocess`) in nnU-Net framework

```bash
docker run --cpus=8 --memory=64gb --shm-size=64gb --gpus='"device=0"' --rm -v /path/to/workdir:/workdir/ yuanyuan29/z-ssmnet:latest nnunet plan_train Task2302_z-nnmnet /workdir/ --trainer myTrainer_zonal --fold 0 --custom_split /workdir/nnUNet_raw_data/Task2302_z-nnmnet/splits.json --plan_only
```

* Perform the same pre-processing on the zonal mask as the corresponding image and save them  in `.npz` format to the `nnUNet_preprocessed` folder. Here, we use `sudo` because we don't have write permission in the folder generated by nnUNet. When using python in the sudo environment, there may be missing modules that need to be installed via `pip`.

```bash
sudo python3 src/z_ssmnet/z_nnmnet/zonal_mask_npz.py 
```

### Z-nnMNet - Training

For general documentation on how to train nnUNet models, please check the [official documentation](https://github.com/MIC-DKFZ/nnUNet#usage). We use a wrapper around nnUNet to orchestrate the `nnUNet_plan_and_preprocess` and `nnUNet_train` steps. If fine-tuning the model based on the SSL pre-training, you need to add the cmd `--pretrained_weights /workdir/SSL/pretrained_weights/ssl_mnet_zonal.model` at the end.

```bash
docker run --cpus=8 --memory=64gb --shm-size=64gb --gpus='"device=0"' --rm -v /path/to/workdir:/workdir/ yuanyuan29/z-ssmnet:latest nnunet plan_train Task2302_z-nnmnet /workdir/ --trainer myTrainer_zonal --fold 0 --custom_split /workdir/nnUNet_raw_data/Task2302_z-nnmnet/splits.json --disable_validation_inference
docker run --cpus=8 --memory=64gb --shm-size=64gb --gpus='"device=1"' --rm -v /path/to/workdir:/workdir/ yuanyuan29/z-ssmnet:latest nnunet plan_train Task2302_z-nnmnet /workdir/ --trainer myTrainer_zonal --fold 1 --custom_split /workdir/nnUNet_raw_data/Task2302_z-nnmnet/splits.json --disable_validation_inference
docker run --cpus=8 --memory=64gb --shm-size=64gb --gpus='"device=2"' --rm -v /path/to/workdir:/workdir/ yuanyuan29/z-ssmnet:latest nnunet plan_train Task2302_z-nnmnet /workdir/ --trainer myTrainer_zonal --fold 2 --custom_split /workdir/nnUNet_raw_data/Task2302_z-nnmnet/splits.json --disable_validation_inference
docker run --cpus=8 --memory=64gb --shm-size=64gb --gpus='"device=3"' --rm -v /path/to/workdir:/workdir/ yuanyuan29/z-ssmnet:latest nnunet plan_train Task2302_z-nnmnet /workdir/ --trainer myTrainer_zonal --fold 3 --custom_split /workdir/nnUNet_raw_data/Task2302_z-nnmnet/splits.json --disable_validation_inference
docker run --cpus=8 --memory=64gb --shm-size=64gb --gpus='"device=4"' --rm -v /path/to/workdir:/workdir/ yuanyuan29/z-ssmnet:latest nnunet plan_train Task2302_z-nnmnet /workdir/ --trainer myTrainer_zonal --fold 4 --custom_split /workdir/nnUNet_raw_data/Task2302_z-nnmnet/splits.json --disable_validation_inference
```

After preprocessing is done, the other folds can be run sequentially or in parallel with the first fold (change the `--gpus` flag accordingly).

### Z-nnMNet - Inference

After training Z-nnMNet to convergence (i.e., after 500 epochs), we can perform inference using Z-nnMNet's 'best' model, `model_best`, or the final checkpoint, `model_final`. We can use a single model for cross-validation, or ensemble the models for the test set.

To evaluate individual models for cross-validation:

```bash
docker run --cpus=8 --memory=28gb --gpus='"device=0"' --rm
    -v /path/to/test_set/images:/input/images
    -v /path/to/test_set/zonal_mask:/input/zonal_mask
    -v /path/to/workdir/results:/workdir/results
    -v /path/to/workdir/predictions:/output/predictions
    yuanyuan29/z-ssmnet:latest nnunet predict Task2302_z-nnmnet
    --trainer myTrainer_zonal
    --fold 0 --checkpoint model_best
    --results /workdir/results
    --input /input/images/
    --output /output/predictions
    --store_probability_maps
    --lowres_segmentations /input/zonal_mask
```

Repeat this for each fold (change `validation_set/fold_1`, `predictions_fold_1` and `--fold 1`, etc.)

### Z-nnMNet - Evaluation

To evaluate, we can use the `picai_eval` repository, see [here](https://github.com/DIAGNijmegen/picai_eval) for documentation.
The Z-nnMNet framework generates _softmax predictions_, while we need _detection maps_ for the PI-CAI challenge. To extract lesion candidates from softmax predictions, we can use the [Report-Guided Annotation repository](https://github.com/DIAGNijmegen/Report-Guided-Annotation). This repository has a [dynamic lesion extraction method](https://github.com/DIAGNijmegen/Report-Guided-Annotation/blob/main/src/report_guided_annotation/extract_lesion_candidates.py), described in [[2]](https://arxiv.org/abs/2112.05151). The lesion candidates from this method are (by design) compatible with the PI-CAI evaluation pipeline. The Report-Guided Annotation repository should be automatically installed when installing the `Z-SSMNet` repository, and is installed within the `z-ssmnet` Docker containers as well.

The Z-nnMNet softmax predictions (saved as .npz files) cannot be used directly, because these pertain to the cropped/preprocessed images, instead of the original images. We use the helper function in [picai_baseline/nnunet/softmax_export.py](https://github.com/DIAGNijmegen/picai_baseline/blob/main/src/picai_baseline/nnunet/softmax_export.py) to convert a cropped prediction to its original extent.

All of the above steps are combined in [z_ssmnet/z_nnmnet/eval.py](https://github.com/yuanyuan29/Z-SSMNet/blob/master/src/z_ssmnet/z_nnmnet/eval.py), enabling evaluation with Docker:

```bash
docker run --cpus=4 --memory=16gb --rm -v /path/to/workdir/:/workdir -v /path/to/repos/:/repos yuanyuan29/z-ssmnet:latest python3 /repos/Z-SSMNet/src/z_ssmnet/z_nnmnet/eval.py --task=Task2302_z-nnmnet
```

The metrics will be displayed in the command line and stored to `metrics-{checkpoint}-{threshold}.json` (by default in `/path/to/workdir/results/nnUNet/3d_fullres/Task2302_z-nnmnet/myTrainer_zonal__nnUNetPlansv2.1/fold_[0,1,2,3,4]`). To see additional options and default parameters, please refer to the command line help (`python src/z_ssmnet/z_nnmnet/eval.py -h`) or the [source code](https://github.com/yuanyuan29/Z-SSMNet/blob/master/src/z_ssmnet/z_nnmnet/eval.py).

To further facilitate false positive reduction, we exclude candidate lesions selected by the [dynamic lesion extraction method](https://github.com/DIAGNijmegen/Report-Guided-Annotation/blob/main/src/report_guided_annotation/extract_lesion_candidates.py) that do not originate from the prostate area. The prostate area is roughly regarded as the composition of zonal masks.

All of the steps for evaluation are combined in [z_ssmnet/z_nnmnet/eval_zonal.py](https://github.com/yuanyuan29/Z-SSMNet/blob/master/src/z_ssmnet/z_nnmnet/eval_zonal.py), enabling evaluation with Docker:

```bash
docker run --cpus=4 --memory=16gb --rm -v /path/to/workdir/:/workdir -v /path/to/repos/:/repos yuanyuan29/z-ssmnet:latest python3 /repos/Z-SSMNet/src/z_ssmnet/z_nnmnet/eval_zonal.py --task=Task2302_z-nnmnet
```

The metrics will be displayed in the command line and stored to `metrics-{checkpoint}-{threshold}-zonal.json` (by default in `/path/to/workdir/results/nnUNet/3d_fullres/Task2302_z-nnmnet/myTrainer_zonal__nnUNetPlansv2.1/fold_[0,1,2,3,4]`). To see additional options and default parameters, please refer to the command line help (`python src/z_ssmnet/z_nnmnet/eval_zonal.py -h`) or the [source code](https://github.com/yuanyuan29/Z-SSMNet/blob/master/src/z_ssmnet/z_nnmnet/eval_zonal.py).

To load the metrics for subsequent analysis, we recommend loading the metrics using `picai_eval`, this allows on-the-fly calculation of metrics (described in more detail [here](https://github.com/DIAGNijmegen/picai_eval#accessing-metrics-after-evaluation)):

```python
from picai_eval import Metrics

fold = 0
checkpoint = "model_best"
threshold = "dynamic"
metrics = Metrics(f"/path/to/workdir/results/nnUNet/3d_fullres/Task2302_z-nnmnet/myTrainer_zonal__nnUNetPlansv2.1/fold_{fold}/metrics-{checkpoint}-{threshold}.json")
print(f"PI-CAI ranking score: {metrics.score:.4f} " +
      + f"(50% AUROC={metrics.auroc:.4f} + 50% AP={metrics.AP:.4f})")
```

## References

[[1]](https://www.nature.com/articles/s41592-020-01008-z) F. Isensee, P. F. Jaeger, S. A. A. Kohl, J. Petersen, and K. H. Maier-Hein, “nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation,” Nature Methods, vol. 18, no. 2, pp. 203-+, Feb, 2021.

[[2]](https://arxiv.org/abs/2112.05151) J. Bosma, A. Saha, M. Hosseinzadeh, I. Slootweg, M. de Rooij, Henkjan Huisman. "Semi-supervised learning with report-guided lesion annotation for deep learning-based prostate cancer detection in bpMRI". arXiv:2112.05151.
